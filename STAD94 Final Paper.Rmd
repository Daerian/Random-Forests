---
title: "A Look Into The Random Forest Algorithm And It's Applications"
author: "Daerian Dilkumar, Guan Yu Chen, Derek Frempong, Zong Li"
date: "April 4, 2018"
document-class: report
header-includes:
  - \usepackage{bbm}
output:
  pdf_document: 
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage
\tableofcontents
\newpage


\section{Introduction}

This project was aimed at building and using an algorithm that will create predictions for classification and regression problems by using the idea of "random forests." The algorithm heavily relies on the aspect of "randomness" to reduce bias, since it attempts to randomly select sample spaces AND feature spaces from the original data, and does it hundreds of times over. The goal is to then to train a series of models and gain inputs from each, contributing to a final result. The algorithm does best at predicting the regressive variant for quantitative predictions. 

A "random forest" is a collections of decision trees which each perform predictions, after which the forest will aggregate these or select a prediction which it will return. Each tree is trained over a bootstrapped sample of the   training set, with a randomly selected set of variables to create it's feature space. This means that each tree has high bias, however since the forest is intended for use with hundreds of trees, the bias in the overall forest drops significantly lower than other models for this result.


Bootstrapping is the process by which we create a sample of the original data, which has the same size as the original data. This is made possible because the sampling is done with replacement. Therefore, the same observations can be used repeatedly, with a uniform distribution chance to be chosen.

\section{Methodologies}

The procedure is as follows:
\begin{itemize}
  \item Select a number of trees for the forest, and how big the feature space is. 
  \item For each tree, choose variables from the feature space, and get a bootstrap. 
  \item Train the model for each tree with these variables for the given labels. 
  \item Collect the trees into a forest to get them ready for predictions 
  \item Finally, given testing data, use the forest to create as many predictions are there are trees.
  \item Aggregate the predictions for a final prediction for each observation in the testing data.
\end{itemize}  

The libraries we used are as follows: 
\begin{itemize}
  \item tidyverse
  \item dplyr
  \item rpart
  \item rpart.plot
\end{itemize}

\section{Development Process}
Let us begin with why our team decided to go with Random Forest. Daerian and Derek went to a DataFest around May 2017 and several of the winning teams used Random Forest in their analysis of the data they were working on. So inspired by the students at the DataFest, we decided to learn about Random Forest, and ultimately implement the algorithm in R on various datasets and see how it fares.

We mainly used the "Elements of Statistical Learning" and "Introduction of Statistical learning with Applications in R" (Gareth, n.d.). Before we even got into Random Forest, we needed to learn about the statistical methods and algorithms used to implement a Random Forest. We found out that Random Forest use an amalgamation of bootstrapped decision trees that sample (without replacement) the predictors to reduce bias and increase variance.

Firstly, we create a function that makes a bootstrapped sampled tree with sampled predictors that $m\leq p$ where $p$ are the number of predictors. We then have a fuction that loops and creates $B$ amount of trees, creating a "Random Forest". We then put the $B$ trees into a list of lists where the $i$th element is a list with three elements. One being the $i$th bootstrapped tree, second being the anti join of the data and final element being the sample used to generate the the boopstrapped tree. Since bootstrapping each sample could give duplicates. Those antijoins become useful when classifying because when classifying we do something called "out of bag sampling", where when predicting each point, we need to aggregate the trees that did not contain that observation we are predicting as to stay accurate. We had to modify the algorithm many times due toaccuracy being nonsensical and/or low accuracy. and now we are playing with controls to increase the accracy in any way.

\section{Examples And Results}

\textsc{Note:} We will be working with our functions and algorithms, which are stored in the file \texttt{RandomForest.R} so we will be first setting this source before we proceed.

```{r, echo = FALSE, message=FALSE, warning=FALSE}
source("randomforest.R")
library(dplyr)
library(tidyverse)
library(rpart)
library(rpart.plot)
set.seed(1)
```

\subsection{Wine Data}

We will create an example using Wine Data. This data performs well using a regression model.
The data is formatted so that the quality is the last column of the data set. We will properly read and process the data so that we can run it through our random forest algorithm.
To start we start at $M = 1$ (1 variable is chosen per tree), and $B = 500$ (500 trees).

```{r, message=FALSE, warning=FALSE}
# Read red wine and white wine data from their respective files
redWineData = read_delim("winequality-red.csv", delim = ";")
whiteWineData = read_delim("winequality-white.csv", delim = ";")
```
Now, we that read the data into R ... time to clean the data and set up the data to run the Random Forest!
```{r}
# Remove NA values
redWineData = (redWineData[complete.cases(redWineData),])
whiteWineData = (whiteWineData[complete.cases(whiteWineData),])

# Prep Sets for merging library
redWineData = redWineData %>% mutate(Type = "Red")
whiteWineData = whiteWineData %>% mutate (Type = "White")

# Merge Data and get Final Dataset
wineData = rbind(redWineData,whiteWineData)

# Remove spaces from column names
names(wineData) = gsub(" ","_", names(wineData))

# Convert columns ot factor as needed and remove not needed columns
wineData$quality  = as.factor(wineData$quality)
wineData$Type = as.factor(wineData$Type)
wineData = wineData[,-ncol(wineData)]
```
Let's split the original dataset into training set and testing data. 
```{r, message=FALSE}
# Split data into training data and testing data, 50% and 50% respectively
split = sample(2, nrow(wineData), prob=c(0.5,0.5), replace=TRUE)
training_set = wineData[split==1,]
testing_set = wineData[split==2,]
```
Lets see if this works. 
```{r}
coverage = as.data.frame(table(split))
coverage$Freq[1]/nrow(wineData)
coverage$Freq[2]/nrow(wineData)
```
The percentages are close enough to the split we want. Perfect! Let's get ready to perform regression.
```{r}
# Acquire labels for the training set
training_labels = as.numeric(as.character(training_set$quality))

# Acquire labels for the testing set
testing_labels = as.numeric(as.character(testing_set$quality))
```
Note here that we need \texttt{as.character} since when converting factors with \texttt{as.numeric} it converts the underlying number and not the factor itself. 
```{r}
training_set = training_set[,-ncol(training_set)]
testing_set = testing_set[,-ncol(testing_set)]

# This is the number of trees we want to create
B = 500

# This is the number of variables we want to use
M = 1   

f = PerformRegression(training_set,training_labels,testing_set,testing_labels,B,M)
```

The above results was acheived when we set the \texttt{seed} to 1. As we can see, the $R^2=0.0949576229835895$, let's see how high the $R^2$ can go. Therefore, we came up with a simulation that can give us the best result possible. Since we need a seed to keep the results consistent and reproducible, we looped through numbers 1 through 100 to set as possible seeds. First, we split the training set and testing set 50\% and 50\% respectively. this give us around the same number as when we set the training and testing set to 75\% and 25\% respectively. We decided to report the bigger number (obviously) which is $R^2=  0.277918789503239$ and $MSE= 0.529555537034112$ when $B=500$ (number of trees), \texttt{seed}$=64$ and $M=5$ (number of variables used per tree). This code can be found in \texttt{experiment.R} (it takes at least 10 minutes to run depending on the parameter). 
```{r, message=FALSE}
set.seed(64)
# Split data into training data and testing data, 75% and 25% respectively
split = sample(2, nrow(wineData), prob=c(0.75,0.25), replace=TRUE)
training_set = wineData[split==1,]
testing_set = wineData[split==2,]

# Acquire labels for the training set
training_labels = as.numeric(as.character(training_set$quality))

# Acquire labels for the testing set
testing_labels = as.numeric(as.character(testing_set$quality))

training_set = training_set[,-ncol(training_set)]
testing_set = testing_set[,-ncol(testing_set)]
M = 5
f = PerformRegression(training_set,training_labels,testing_set,testing_labels,B,M)
```

We can see an improvement in $R^2$ and MSE, but $R^2$ is still very low. One of the reasons that the $R^2$ is low could be the distribution of the data we want to classify. Let's take a closer look at the labels that the Random Forest algorithm was trying to classify, specifically the distribution.
```{r}
#Draw histogram
data <- as.integer(as.character(wineData$quality))
h <- hist(data, breaks=1:10, ylim=c(0, 3000), freq=TRUE, main = "Frequency Histogram", 
          xlab = "Quality")

#Fit curve with normal distribution
xfit<-seq(min(data),max(data),length=40) 
yfit<-dnorm(xfit,mean=mean(data),sd=sd(data)) 
yfit <- yfit*diff(h$mids[1:2])*length(data) 
lines(xfit, yfit, col="red", lwd=2)

# Occurences of labels in training set and testing set
table(training_labels)
table(testing_labels)
```
As we can see from the counting results and histogram, most of the labels are either 5 or 6. Labels 3,4,7,8 and 9 have significantly less occurences in the data, and there are no wine with quality greater than 9. Having this distribution will lead the Random Forest to give a quality $q$, where $q\in [5,7)$ to almost all the wine, since it is more likely to sample a wine with quality 5 or 6 to build the Random Forest than to sample a wine that is not. Let's take a look at a sample decision tree to confirm this hypothesis.
```{r}
sample_dtree = BT_Tree(training_set, training_labels, 5, tree.print=TRUE)
```
We can see that the leaf nodes all have qualities/labels that are in the open interval $[5,7)$. Generating large number of trees won't change this result. Even if there was a few decision trees that predicted other qualities, these predictions would get 'outnumbered' and would be insignificant to the final prediction.

Another reason is the decision trees that are used in the algorithm. We are using \texttt{rpart} to produce the trees. This is a built in R function that generates decision trees given a dataset. Since this function is blackbox, we are not sure what exactly it does to separate the feature space to make the trees. If we had more time, we would love to learn more about how to split feature space and produce trees manually that helps us improve the accuracy.

\subsection{Breast Cancer}
Heres another example, but instead we will do classification on this dataset rather than regression. This data is is preprocessed already so we have to read in the data only.

```{r BC read, cache=TRUE, message=FALSE, warning=FALSE}
BreastCancer = read.table("BreastCancer.csv",header=T, sep=",")
```
Now that we have the data we can then perform Classification on it and see how it fares.

```{r BC perform}
# training_set2 = sample_n(BreastCancer, nrow(BreastCancer)/2, replace=FALSE)
# testing_set2 = anti_join(BreastCancer,training_set2)
# labelsBC = as.factor(unlist(training_set2[ncol(training_set2)]))#training_set labels
# labelsBC2 = as.factor(unlist(testing_set2[ncol(testing_set2)]))# testing_set Labels
# training_set2 = training_set2[,-ncol(training_set2)] #getting rid of the labels to  prepare
# # To perform classification
# testing_set2 = testing_set2[,-ncol(testing_set2)]
# B2 = 500
# M2 = 3
# f = PerformClassification(training_set2,labelsBC,testing_set2,labelsBC2,B2,M2,BreastCancer)
```


\section{Wrap-Up}

\section{Appendix}
\textsc{Note:} The following libraries are used throughout the code: \texttt{tidyverse}, \texttt{dplyr}, \texttt{rpart}, and \texttt{rpart.plot}.


\section{References}
<!-- in-text: (Gareth, n.d.) -->
Gareth, J. \textit{An Introduction to Statistical Learning: with Applications in R}. Springer.

<!-- in-text: (Hastie, n.d.) -->
Hastie, T. \textit{The elements of statistical learning [electronic resource] : data mining, inference, and prediction} (2nd ed.). Springer.

<!-- in-text: ("Random forest", 2018) -->
\textit{Random forest}. (2018). \textit{En.wikipedia.org}. Retrieved 22 January 2018, from [https://en.wikipedia.org/wiki/Random_forest](https://en.wikipedia.org/wiki/Random_forest)